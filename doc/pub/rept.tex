\documentclass[11pt,a4paper,twoside,openright,draft]{report}

% The PDF must be named rep_grochmalm_act.pdf

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}

\usepackage{parskip}
\usepackage{multicol}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{fancyvrb}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tocloft}
\usepackage{graphicx}
\usepackage{rotfloat}
\usepackage{multirow}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage[table]{xcolor}

% \label convention
% chap:    chapter
% sec:     section
% subsec:  subsection
% fig:     figure
% tab:     table
% eq:      equation
% lst:     code listing
% itm:     enumerated list item
% app:     appendix subsection

\title{Style Based Drawn Artwork Image Classification}
\author{A dissertation submitted in partial fulfilment of the requirements\\
  for the MSc in Intelligent Technologies\\
  \\
  by Michal Grochmal
  $<$\href{mailto:grochmal@member.fsf.org}{grochmal@member.fsf.org}$>$\\
  supervisor Dell Zhang
  $<$\href{mailto:dell@dcs.bbk.ac.uk}{dell@dcs.bbk.ac.uk}$>$\\
  \\
  Department of Computer Science and Information Systems\\
  Birkbeck College, University of London
}
\date{September 2014}

\usepackage[colorlinks=true]{hyperref}

%\hyphenation{ge-ne-ric Ge-ne-ric Ro-me-ro Ma-ria}

% fix adobe's PDF reader (it cannot stand xcolor)
\definecolor{lightgray}{gray}{0.7}
\setlength{\headheight}{16pt}
\setlength{\arrayrulewidth}{0.6pt}

%\renewcommand{\chaptername}{Section}  % Not needed
\renewcommand{\bibname}{References}
\titlespacing*{\chapter}{0pt}{60pt}{60pt}

\titleformat{\chapter}[display]{
  \normalfont\huge\bfseries}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titleformat{\section}[block]{
  \normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}[block]{
  \normalfont\large\bfseries}{\thesubsection}{1em}{}

\begin{document}
\VerbatimFootnotes
\maketitle

\newpage
\null
\thispagestyle{empty}
\newpage

\pagestyle{fancy}
\lhead{}
\chead{STYLE BASED DRAWN ARTWORK IMAGE CLASSIFICATION}
\rhead{}

\begin{abstract}

The abstract of the work, this will be written at the very end

\begin{flushright}
\emph{Ars longa, vita brevis -- Hippocrates}
\end{flushright}
\end{abstract}

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Table of Contents}
\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{List of Figures}
\listoffigures

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{List of Tables}
\listoftables

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Academic Declaration}
\chapter*{Academic Declaration}
This report is substantially the result of my own work except where explicitly
indicated in the text.  I give my permission for it to be submitted to the JISC
Plagiarism Detection Service.  I have read and understood the sections on
plagiarism in the Programme Handbook and the College website.

The report may be freely copied and distributed provided the source is
explicitly acknowledged.

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{Acknowledgements}
\chapter*{Acknowledgements}
Thanks to my family and my friend, for their patience with me not having the
time to spend with them.

\newpage
\null
\thispagestyle{empty}
\newpage

\newpage
\setcounter{page}{1}
\pagenumbering{arabic}

\chapter{Introduction}
\begin{multicols}{2}

Similar images are classified by Content Based Image Retrieval systems (CBIR
systems) based on \emph{several features}.  The feature we use depend on one of
the meanings (semantics) contained in the image, it could be one specific
object present in the image, a combination of objects, a specific person or
something even different.  One image has often much more than one meaning.
e.g. we might query a collection of personal photos using a picture of our
cousin holding a carnival mask in Venice.  For this query image we might ask:
whether there are photos of our cousin in the collection, or whether there are
photos of venetian carnival masks, or photos with a specific building or
location in Venice in the background.

Different \emph{sets of features} are needed for different semantics and
different collections.  In this work we will search for features related to the
\emph{artistic style} between different artists and, more generally between
different art schools (aka. art movements).  We will focus on \emph{aesthetics}
of an image as it is related to the concept of art \cite{rmc12ajs}, and on the
\emph{emotions} an image can provoke on the viewer as it is one of the purposes
of art \cite{mach10clas}.

CBIR systems for artwork \cite{cfsp12air,isv12mpeg,ymvz03tree} can benefit from
extra features and from a better understanding of current features.  Museum
collection can also benefit from better classification of art images:  Several
images in these collections miss some or all metadata, by finding similar
images we could fill in the missing metadata.

\section{Background}

Features that are independent for objects in the image (or at least as
independent as possible) need to be used to evaluate the art style.  Work by
Zirnhelt \cite{zirnhelt07art} present a limited number of such features in the
\emph{context of art}.  Works by Machaijdik \cite{mach10clas} and Romero
\cite{rmc12ajs} present more features independent from objects, but most of the
features tried were tested on illustration and photography only.  Romero in
\cite{rmc12ajs} tests some of the features on a very limited number of art
paintings.  Thanks to photo web sharing (\texttt{photo.net},
\texttt{panoramio.com} or \texttt{flickr.com}) consistent datasets of
photography are much easier to acquire than datasets of art, that is the major
reason why the majority of work on style uses photography.

Art paintings style is evident to humans, Figures \ref{fig:diff} and
\ref{fig:similar} show an example of image similarity.  In Figure
\ref{fig:diff} we see paintings by Francesco Granacci and by Francesco
Pesellino, these paintings contain similar object and depict a similar contex
yet their \emph{style is different}.  Figure \ref{fig:similar} shows two
paintings by Delacroix, in which the \emph{style is similar} although the
scenes depicted are different.  Using the correct features we shall be capable
of identifying the firs two images as different and the last two as similar.
[The images are part of the public collection of the Victoria and Albert Museum
in London, this collection is used as a dataset in this work.  See Chapter
\ref{chap:data}.]

To achieve such classification we follow works that detour from interest points
or corner detection for general object identification as SIFT or MSER
techniques \cite{szel11book}.  Such features describe the content of the image
instead of it's style.

\begin{figure*}[tb]  % figure* instead of figure because of multicols
\centering
\includegraphics[width=0.48\textwidth]{diff_caesar}
\includegraphics[width=0.48\textwidth]{diff_horatius}
\caption[Example of different styles]{The image to the left is the painting
"Julius Caesar and the Crossing of the Rubicon" by \emph{Francesco Granacci}
and to the right is the panel "Horatius Cocles Defending the Sublician Bridge"
by \emph{Francesco Pesellino}.  The theme in both images is the same: roman
warfare, soldiers and horses moving through a river.  Yet the style used in
depicting both scenes is different.}
\label{fig:diff}
\end{figure*}

\begin{figure*}[tb]
\centering
\includegraphics[width=0.48\textwidth]{sim_delacroix_samaritan}
\includegraphics[width=0.48\textwidth]{sim_delacroix_shipwreck}
\caption[Example of similar styles]{A similar style can be observed in these
two paintings by \emph{Eug\`ene Delacroix}: "The Good Samaritan" (left) and
"The Shipwreck of Don Juan" (right).  The author's style of painting is
observed in both paintings, although the theme of the images is completely
different.}
\label{fig:similar}
\end{figure*}

\section{Features in the previous works}

A richer sets of features can be found in works that explore style in
photography \cite{jma12clas,cmrc13fs,rmc12ajs,mach10clas}.  \emph{Image
complexity} based on lossy compression, \emph{standard deviation} and
\emph{average} over components of \emph{HSV} and/or \emph{HSL} colour
representation, \emph{Itten colours and contrasts} and weighted importance of
all these feature in the center of the image.  Other works propose histograms
of black and white on drawing classification \cite{kroner98draw} or MPEG-7
descriptors as features.

Size normalisation of all images is needed as all these features are
\emph{scale dependent} \cite{jma12clas,mach10clas}.  Also, several of the works
(\cite{jma12clas,cmrc13fs,rmc12ajs}) perform colour normalisation to eliminate
the differences of illumination between images.  RGB colour space do not
represent well the human perception of the image \cite{mach10clas} therefore
the features are extracted from the \emph{HSV} and \emph{HSL} colour
representations of the image.

In this work, to the features used by Romero in \cite{rmc12ajs} we add many
features described by Machajdik in \cite{mach10clas}.  Although we ignore some
of Machajdik's features that are directly related to object recognition as we
need features that are as independent as possible form the objects in the
image.  From \emph{Romero's work} we use Kolmogorov complexity features and
image averages, from \emph{Machajdik's work} we use Itten colours, Itten
contrasts, grey level co-occurrence matrix and the rule of thirds.  The most
complex of these features to implement are the Itten contrasts.  In Romero's
and Machajdik's work the task of author classification is not the main focus,
we use their features on a much bigger collection as a main focus of this work.
We also extrapolate the \emph{feature extraction and classification} to
classify on artistic schools.

\section{Project execution}

Time and effort to acquire the datasets proved to be the most complicated part
of the work.  It is not surprising that the number of works that explore art
collections is small given that big and consistent datasets are hard to
acquire.  The remaining of the time of the project was spent on feature
extraction, classification and write up.

The \emph{13 weeks} of the project were executed as follows: First 7 weeks were
used for dataset acquisition and cleansing, next 3 week for feature extraction,
one week of holiday, 1 week of classification and 1 week for write up.  The
dataset acquisition work can be subdivided into: \emph{1st week}: reading the
VAM JSON API and building scripts for the VAM dataset crawlers; \emph{2nd
week}: organisation of VAM data, removing inexistent, duplicated, misclassified
paintings and unknown artists;  \emph{3rd week}: writing crawlers for the NIRP
dataset, crawlers over artist indexes and painting indexes which are then
compared against each other;  \emph{4th week}: Word count and manual selection
of artists from the NIRP dataset;  \emph{5th week}: Recording of data from NIRP
dataset, including manual intervention with encoding issues;  \emph{6th week}:
writing unixjsons tools (see Appendix \ref{chap:unixjsons} to deal with data
organisation;  \emph{7th week}: using unixjsons tools to filter only the images
of artists that have a sizeable amount of paintings, also a manual check on the
resulting images.

Feature extraction can be subdivided into: \emph{8th week}: normalisation,
separation and Kolmogorov complexity features;  \emph{9th week}: GLCM features,
rule of thirds and image averages; \emph{10th week}: Watershed segmentation and
Itten colours and contrasts.

Remaining two weeks were used to link metadata to the features and run the
classifiers.  The last week was mostly used to finish the report write up.

\section{Summary}

Chapter \ref{chap:data} describes the process of dataset acquisition, for each
of the two datasets used.  Chapter \ref{chap:approach} extensively describes
the feature extraction process, dividing the features into related groups and
explaining each.  Chapter \ref{chap:results} describes a crude classification
process as an example of how these features can be used.  Appendices show how
to use the code accompanying this report and show the most important parts of
the data and code used.

\end{multicols}

\chapter{Data}
\label{chap:data}
\begin{multicols}{2}

We use online art collections as the datasets for our classification,
unfortunately such art collections are far from being machine readable inputs
of images and metadata.  Two such collections are used: the \emph{Victoria and
Albert Museum (VAM)} collection\footnote{\href{http://www.vam.ac.uk/}
{http://www.vam.ac.uk}}, which is freely available for academic use.  And the
\emph{NICE paintings (NIRP)} collection available from the Visual Arts Data
Service (VADS)\footnote{\href{http://vads.ac.uk/}{http://vads.ac.uk}} project.
NIRP collection is an agglomeration of several collections of paintings
including, among others, the CACHe project\footnote{
\href{http://www.bbk.ac.uk/hosted/cache/}{http://www.bbk.ac.uk/hosted/cache}}
at the Birkbeck College.

To produce a machine readable dataset we crawl the NIRP and VAM collections for
paintings, then we filter them based on the completeness and \emph{consistency
of the metadata} of the collection item.  This task often cannot be done
automatically, e.g. we rejected a painting by Leonardo da Vinci painted in
1856, 200 years after the painter's death.  In this case it is clear that the
metadata of the item is incorrect and the painting cannot be used, but writing
a piece of software that evaluates such metadata to be correct or incorrect is
a project in it's own.  Therefore, a lot of the work on the acquisition of the
collection items was done manually as item with wrong metadata would add extra
complexity to the classification.

Results of the crawl, the images, are assigned the surname of the artist and a
unique identifier consisting of letters and numbers \texttt{<painter>\_<id>}.
We use more than one dataset, therefore later the images are prefixed with the
dataset name to generate globally unique identifiers:
\texttt{<dataset>\_<painter>\_<id>} (e.g. \texttt{vam\_constable\_2006AP4487}
or \texttt{nirp\_monticelli\_fa000286}).

\section{Victoria and Albert Museum (VAM)}

In the VAM collection we are interested in the collection objects classified as
paintings.  The entire collection contains other objects that are not
paintings: sculpture, clothing or manuscripts among others.  In the VAM
collection there are 2000 items classified as paintings.  VAM exposes a JSON
API\footnote{\href{http://www.vam.ac.uk/api/json/museumobject/}
{http://www.vam.ac.uk/api/json/museumobject/}} to retrieve metadata and image
web locations, the metadata is organised as a small JSON structure for each
museum item.

The API do not allow to retrieve data for more than 45 items in one call,
therefore we develop a crawling script (\texttt{vam-get-all.py}) that retrieves
data about the 2000 paintings in order.  Instead of working with big JSONs by
joining all JSON results into one agglomerated result we take the approach of
Chen in \cite{chen09yahoo} and use a plain text file containing one small JSON
per line.  Each line in this JSONS file is the JSON encoded data for one
painting, as Chen \cite{chen09yahoo} describes, this approach reduces the
memory needed to process each record and allows the processing of the entire
dataset as a stream.

To easily work with files in the JSONS, format we developed implementations of
the \texttt{grep}, \texttt{cut}, \texttt{sed}, \texttt{echo} and \texttt{join}
commands over files and streams in the JSONS format.  This set of tools is
described in Appendix \ref{chap:unixjsons}.

Using \texttt{jgrep} we filtered out of the collection items classified as
paintings all items that did not have and image.  This left us with 878 items
with an image.  We then downloaded the images for these items using another
crawling script (\texttt{vam-img-get.py}).  Some of the images did not depict
paintings, therefore we manually went through each of the 876 downloaded images
and removed items that were misclassified as paintings.  We found 4 images of
vases, 3 images of sculptures and 3 references to a photo of three vials of
paint (which also explained why we got 876 images for 878 items).  We removed
all misclassified items from the set and 868 paintings remained.

Several paintings had the "artist" metadata field set as: "Unknown", "unknown"
or "" (empty).  We removed all these paintings from the set as we would not be
able to evaluate the classification according to \emph{artist identification}
if we do not know the artist in the first place.  303 were removed from the set
for having the artist unknown, leaving 565 paintings with assigned artists.

Next we needed to remove artists that have a number of paintings too little to
be relevant.  A classifier that can assign the only painting of a certain
artist to that artist is not a general classifier.  Yet first, the metadata in
the "artist" field needed to be cleaned to find the names of artists and the
paintings by each artist.  e.g. the name of Charles Robert Leslie is encoded in
the VAM collection as both "Charles Robert Leslie" and "Leslie, Charles Robert
(RA)" of the name of William Mulready is encoded as "Mulready, William (RA)"
and as "Mulready, William".  A simple \texttt{jsed} script
(\texttt{cleanse-artists.jsed}) was created to re-encode the artist's names in
a consistent manner across the set of 565 paintings.

After the cleaning of artist names we got a list of 281 artists, most of them
with only one painting in the collection.  We filtered out all artists with 6
paintings or less from the set and collected the paintings for all remaining
artists.  This leaves the \emph{VAM dataset} with 193 paintings by 14 artists:
John Constable (39 paintings), Thomas Gainsborough (13 paintings), Charles
Robert Leslie (10 paintings), William Mulready (10 paintings), George Frederick
Watts (10 paintings), William Carpenter (9 paintings), Abanindranath Tagore (7
paintings), Basawan (11 paintings), Jagan (9 paintings), Kesav Kalan (15
paintings), La'l (19 paintings), Miskin (15 paintings), Tulsi Kalan (10
paintings) and Devi (16 paintings).

Table \ref{tab:crawl} shows the VAM dataset acquisition in perspective.

\begin{table*}[ptb]
\centering
\rowcolors{1}{}{lightgray}
\begin{tabular}{|l|rr|}
\toprule
Dataset & VAM dataset & NIRP dataset \\
\midrule
Paintings available                   & 2000 & 24373 \\
Unique paintings (according to index) & 2000 &  9181 \\
Paintings without an image            & 1132 &  2511 \\
Paintings with an image               &  878 &  6641 \\
Object misclassified as painting      &   10 &     1 \\
Corrupted image file                  &    0 &     8 \\
Duplicate image                       &    0 &     5 \\
Different photo of the same painting  &    0 &    15 \\
Artist of painting was not known      &  303 &   144 \\
% VAM had "Unknown", "unknown" and ""
% NIRP was a mess, as always
\midrule
Indexed artists                       &  281 &  3222 \\
Missing artist index                  &    0 &     1 \\
Not artist index entry                &    0 &     9 \\
Artists without paintings             &    0 &    23 \\
Artists used for selection            &  281 &  3190 \\
Selected artists                      &   14 &    42 \\
\midrule
Paintings by the selected artists     &  193 &  1007 \\
\bottomrule
\end{tabular}
\caption[Dataset acquisition]{Dataset acquisition.  The number of images and
artists at each phase of crawling and cleansing the dataset.}
\label{tab:crawl}
\end{table*}

\section{Visual Arts Data Service - NICE Paintings (NIRP)}

NIRP is an agglomeration of public collections in the UK and it contains more
than 9000 pre-1900 European \emph{oil paintings}.  Unfortunately the
organisation of the collection is poor, and no API exists.  The crawlers
collection the data from the NIRP collection are \emph{HTML crawlers}, yet to
maintain consistency with the VAM dataset the final form of the metadata for
the NIRP collection are plain files containing a JSON on each line.

Although no API exist for the NIRP dataset there is an alphabetical index of
artist names.  With a script (\texttt{all-artists.py}) we crawled the index of
artist names producing a list of all 3222 artists in the NIRP collection.
A manual intervention was needed in the crawl as the index for the letter "x"
exists although there is no artist name starting with the letter "x".  From the
remaining index entries we manually removed 9 entries that were not artists but
agglomerations of artists by country (e.g. "French Painters" under the letter F
or "Italian Painters" under the letter I).

Each index entry links the artist name to a query search string using the
VADS's website\footnote{\href{http://www.vads.ac.uk/collections/NIRP/index.php}
{http://www.vads.ac.uk/collections/NIRP/index.php}} search engine.  The search
retrieves matches on all words of the query, therefore the index links of the
three Italian artists: Francesco Bassano, Leandro Bassano, and Jacopo Bassano
il vecchio result in the same list of paintings for each.  Whether an artist
shares part of his name with another artist the result for both are displayed
when following the index link for any of them, e.g. every John is assigned the
paintings of all Johns in the NIRP collection.

The descriptions of paintings in the NIRP collection posses \emph{unique IDs},
therefore we collected all painting descriptions from all artists in the artist
name index including several duplicates.  With the duplicates we had 24 373
painting descriptions, filtering them by the \emph{unique IDs} resulted in 9181
unique painting descriptions.  Also, from the index of artist names we manually
removed 23 artists for which the index did not return any paintings, duplicates
of not.  This was checked using a combination of \texttt{curl} and
\texttt{grep} calls.

From the 9181 descriptions of paintings only 2511 had available images, which
we downloaded using a \texttt{get-painting.py} script.  The remaining 6641
painting descriptions were removed from the set, as we need an image of the
painting to extract features from.  A script (\texttt{by-artist.py}) was used
to crawl the descriptions of the 2511 paintings with an image, yet it did not
produce acceptable results.  The encoding of the HTML encoding used by VADS is
\texttt{UTF-8} but several pieces of metadata are encoded in \texttt{ISO
8859-1} character encoding, sometimes in \texttt{ISO 8859-2} character encoding
and some (not all) quotes are encoded in \texttt{Windows-1252} character
encoding.  The main piece of metadata which had mixed encodings across
different painting descriptions was the artist field, the most important
metadata field for this project.  The \texttt{by-artist.py} was extended to
deal with mixed encoding of \texttt{UTF-8} and \texttt{ISO 8859-1}, but the
cases where characters were encoded in \texttt{ISO 8859-2} or
\texttt{Windows-1252} needed to be dealt with manually.  After all changes, all
painting descriptions were converted to \texttt{UTF-8} character encoding and
formatted in the same way as metadata from the VAM collection (a JSONS file).

In possession of 2511 paintings with descriptions there were 3190 artists to
assign to them based on a messy artist name index.  The (now UTF-8 encoded)
data for the "artist" field was not possible to clean with a \texttt{jsed}
script as in case of the VAM collection.  Most of the data was encoded in
natural language, for example:

\begin{quote}
"artist": " Attributed to school of Masaccio (Italian painter, 1401-1428)
Attributed to Giovanni di Ser Giovanni Guidi (Italian painter, 1406-1486)
Previously attributed to Boccati, Giovanni di Piermatteo da Camerino (Italian
painter, born ca.1420, active 1480) "
\end{quote}

Even unknown artists were not stated in a machine readable way:

\begin{quote}
"artist": " Unknown  Previously attributed to Soldi, Andrea (Italian painter,
ca.  1703-1771, active in Great Britain) "
\end{quote}

Therefore the number of paintings of unknown artists for the NIRP collection in
Table \ref{tab:crawl} is smaller than the real number.

To find the actual artists of the 2511 paintings we concatenated together the
data of the "artist" field for all and run a word count algorithm over the
result.  The word count was just a combination of \texttt{jcut}, \texttt{cut},
\texttt{sed}, \texttt{sort} and \texttt{uniq} commands.  From the result of the
word count we removed stop words and removed any words appearing less than 10
times (unless the word was clearly the surname of a well known painter).  The
resulting ~2000 words were checked by hand, using \texttt{jgrep} to ensure that
it is a surname or forename of an artist by checking the "artist" metadata
field of each image that cntains that word.  It was a humongous manual work to
organise the words into 42 artist names which had a considerable number of
works in th NIRP collection.

For the selected 42 artist, a script (\texttt{filter-artists.sh}) was developed
to pick the images of each artist (using regular expressions and
\texttt{jgrep}) and to enhance the metadata with a machine readable "artist"
field.  At the end of the process each artist had a metadata file in JSONS
format and a directory of all images listed in the metadata file.  Finally, the
image file names and IDs were all converted to lowercase, to make them more
consistent.  The total of images for all the 42 artists was 1036.  The names of
all artists are too long to list therefore are presented in Table
\ref{tab:style}, together with the 14 artists from the VAM collection.

The 1036 paintings were checked for duplicates using a \texttt{SHA1} checksum,
resulting in 8 corrupted files and 5 duplicate images (same painting, different
description).  All 13 images were removed leaving 1023 paintings, which were
then checked manually for duplicates.  There were 15 duplicates (almost
identical image of the same painting) and 1 image of a sculpture, which were
all removed from the dataset.  The final dataset from the NIRP collection had
1007 images by 42 artists.  Table \ref{tab:crawl} also shows the perspective of
the acquisition of the NIRP dataset.

\section{Datasets in previous works}

Two of Romeo's and Machado's works (\cite{jma12clas} and \cite{cmrc13fs})
contain datasets of photos only, and these are classified into "good
aesthetics" and "not good aesthetics".  This is not the kind of data we need
for a classification of paintings.  Another of Romeo's works \cite{rmc12ajs}
has a dataset of photos and a small dataset of paintings.  We tried to contact
Romeo and Machado for the dataset, or at least the titles of the paintings they
used, but we received no answer to the request.

Machajdik's work \cite{mach10clas} and \cite{mach10ua} uses images from the
\texttt{deviantart.com} website, it does sepcify that 809 images were used but
does not specify the images used.  \emph{Deviantart} has thousands of images
and sampling 809 at random would likely produce different results.  Finally,
deviantart focuses on digital art, which differs from classical paintings
significantly.

\end{multicols}

\chapter{Approach}  % Why not "Methodology"?
\label{chap:approach}
\begin{multicols}{2}

With the datasets downloaded and organised we join them to achieve the number
of 1200 images of paintings by 56 artists, over which we can perform feature
extraction.  First we normalise all images to similar sizes and separate them
into channels, then we run the feature extractors.  All features have been
divided into related groups: \emph{Kolmogorov complexity}, \emph{grey level
co-occurrence matrix}, \emph{amount of Itten colours}, \emph{rule of thirds},
\emph{Itten contrasts} and \emph{image averages}.  A separate feature extractor
for each group of features was built.

Later (in Chapter \ref{chap:results}) we join all features together and run the
classifiers over the joined data.

\section{Normalisation and separation}

First of all size normalisation, as most features depend on the number of
pixels in the image.  We normalise each image to a size close as possible to
300 000 pixels but without changing the aspect ration, this approach is adopted
by Machajdik \cite{mach10clas}.  In \emph{golden ratio} 300 000 pixels is an
image of 671x447 pixels, which maintains a good amount of detail without being
too big.  Equation \ref{eq:size} shows how an image of arbitrary \emph{height}
and \emph{width} can be resized to an image of 300 000 pixels.  A Perl script
called \texttt{norm-size.pl} was developed to perform this task.  This
technique do not ensure that an image will have the desired number of pixels
but that it will be as close as possible to 300 000 pixels without changing the
aspect ratio.  In our dataset, the image that deviated the most ended with 300
515 pixels, that's an error of less than 0.172\% and, therefore, can be
ignored.

% from libsvm documentation
%
% normalisation to [0,1]:
%   x' = ( x - min(x) ) / ( max(x) - min(x) )
%
% normalisation to [-1,1]:
%   x' = 2 * ( x - min(x) ) / ( max(x) - min(x) ) - 1

\begin{equation}
\begin{aligned}
h'  &= \left\lfloor h \times
                    \sqrt{ \frac{3 \times 10^5}{h \times w} } \right\rfloor \\
w'  &= \left\lfloor w \times
                    \sqrt{ \frac{3 \times 10^5}{h \times w} } \right\rfloor \\
\label{eq:size}
\end{aligned}
\end{equation}

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.48\textwidth]{nirp_caravaggio_1962_139_1}
\includegraphics[width=0.48\textwidth]{caravaggio_1962_139_1}
\includegraphics[width=0.48\textwidth]{nirp_rembrandt_eu_464}
\includegraphics[width=0.48\textwidth]{rembrandt_eu_464}
\caption[Colour normalisation]{To the left are the original images, to the
right normalised ones.  It can be seen that the normalised images are brighter
to the human eye, this is because the darker and brighter colours are further
apart in the normalised images.  The top image is the "The Betrayal of Christ"
by \emph{Michelangelo da Caravaggio} and the image at the bottom is the "The
Anatomy Lesson of Dr. Nicolaes Tulp" by \emph{Rembranidt van Rijn}.}
\label{fig:norm}
\end{figure*}

As done by Romero and Machado in  \cite{jma12clas}, \emph{colour normalisation}
of the images is performed: for each channel, we subtract the minimum value
from all pixels, then divide all pixels by the value of the maximum value and
multiply by 255.  Different pictures of the same painting may have been taken
under different illumination, colour normalisation removes this complexity

\begin{figure*}[!htb]
\begin{equation}
\begin{aligned}
H_{HSV}  &= atan2\left(\frac{\sqrt{3}}{2}(G-B), \frac{1}{2}(2R-G-B)\right) \\
V_{HSV}  &= max(R,G,B) \\
S_{HSV}  &= \left\{
  \begin{array}{ll}
    0  &  \text{if } V = 0, \\
    \frac{\sqrt{ \left(\frac{\sqrt{3}}{2}(G-B)\right)^2
               + \left(\frac{1}{2}(2R-G-B)\right)^2 }}{V}
  &  \text{otherwise}. \\
  \end{array}
            \right.
\label{eq:hsv}
\end{aligned}
\end{equation}
\end{figure*}

\begin{figure*}[!htb]
\begin{equation}
\begin{aligned}
H_{HSL}  &= atan2\left(\frac{\sqrt{3}}{2}(G-B), \frac{1}{2}(2R-G-B)\right) \\
L_{HSL}  &= \frac{1}{2}max(R,G,B) + \frac{1}{2}min(R,G,B) \\
S_{HSL}  &= \left\{
  \begin{array}{ll}
    0  &  \text{if } L \in \{0,1\}, \\
    \frac{\sqrt{ \left(\frac{\sqrt{3}}{2}(G-B)\right)^2
               + \left(\frac{1}{2}(2R-G-B)\right)^2 }}{1 - \lvert 2L-1 \rvert}
  &  \text{otherwise}. \\
  \end{array}
           \right.
\label{eq:hsl}
\end{aligned}
\end{equation}
\end{figure*}

\begin{equation}
CS_{HSV} = \frac{S_{HSV} \times V_{HSV}}{255}
\label{eq:cs}
\end{equation}

The \emph{HSV colour space} was used by Romero \cite{jma12clas} in his work to
extract the features, on the other hand Machajdik \cite{mach10clas} uses the
\emph{HSL colour space} for the features in her work.  Both HSV and HSL colour
spaces are closer to how we humans perceive an image rather than the \emph{RGB
colour space}.  All images in the dataset are in RGB therefore we use
ImageMagick\footnote{ \href{http://imagemagick.org/}{http://imagemagick.org/} }
to convert the images into HSV and HSL colour spaces, the conversions are
depicted by Equation \ref{eq:hsv} and Equation \ref{eq:hsl}.  Each channel
(\emph{Hue}, \emph{Saturation}, \emph{Value} and \emph{Lightness}) is stored as
a Portable Graymap (PGM)\footnote{One of the \texttt{netpbm} formats for images
(PPM, PGM and PBM)}.  This format stores the channel uncompressed, which is
important for the calculation of the Kolmogorov complexity (one of the intended
features).  From the Equations \ref{eq:hsv} and \ref{eq:hsl} we can see that
the Hue is the same for both HSV and HSL colour spaces, yet the Saturation is
different.  We, therefore, store 5 channels (each as a separate PGM) for each
image: \emph{Hue}, \emph{Saturation$_{HSV}$}, \emph{Saturation$_{HSL}$},
\emph{Value}, and \emph{Lightness}.  Each channel is separated from the
original image as a single operation due to a limitation where ImageMagick
concatenates all channels into the output image when extracting three channels
at the same time.

In \cite{rmc12ajs} Romero defines an extra image channel from which he extracts
features, the \emph{colourfulness of the image}.  The colourfulness is the hue
of the image damped by the saturation, in the HSV colour space.  To achieve the
colourfulness the hue and saturation are multiplied and scaled to a range
between 0 and 255.  We developed a script (\texttt{sv2cs.py}) that performs the
calculation of the colourfulness of the image from the hues and saturation
according to the Equation \ref{eq:cs}.  Note that ImageMagick already scales
the hue channel to values between 0 and 255 when separating the channels,
therefore the Equation results in values in the range between 0 and 255.

Feature extractors then run on some or all of the six "channels" of the image:
\emph{Hue}, \emph{Saturation$_{HSV}$}, \emph{Saturation$_{HSL}$}, \emph{Value},
\emph{Lightness} and \emph{Colourfulness}.  Although these are not strictly
image channels, because the combination of them do not form the original image,
we will refer to these six representations as "image channels" throughout the
rest of the document for simplicity.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.30\textwidth]{H_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{SHSV_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{V_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{H_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{SHSL_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{L_caravaggio_1962_139_1}
\caption[HSV/HSL separation]{HSV/HSL separation of "The Betrayal of Christ".
At the top are the channels \emph{Hue}, \emph{Saturation} and \emph{Value} of
the HSV representation, at the bottom are the \emph{Hue}, \emph{Saturation} and
\emph{Lightness} channels of the HSL representation.  The \emph{Hue} channel is
the same in both representations, whilst the \emph{Saturation} and the third
channel is different across the representations.}
\label{fig:hsvl}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.30\textwidth]{H_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{SHSV_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{CS_caravaggio_1962_139_1}
\caption[Colourfulness]{The \emph{Colourfulness} (right) of an image can be
derived from it's \emph{Hue} channel (left) and it's \emph{Saturation} channel
(centre), taken from the HSV representation.  Both channels are multiplied and
scaled as seen in equation \ref{eq:cs}.}
\label{fig:cs}
\end{figure*}

\section{Basic Features}

Some features shall be retrieved from each of the six image channels.  From
Romero's work in \cite{rmc12ajs} we extract the features the are an
approximation of \emph{Kolmogorov complexity} of each of the channels; used in
Machajdik's work \cite{mach10clas} and Zirnhelt's work \cite{zirnhelt07art} the
features based on texture are calculated on the \emph{Grey Level Co-occurrence
Matrix} of each channel.  The \emph{average and standard deviation of all
pixels} in each of the channels is a common feature used across all three works
\cite{jma12clas,zirnhelt07art,mach10clas}.  Machajdik \cite{mach10clas} also
proposes to use the average and standard deviation of the pixels in the central
rectangle of the image when it is divided into three parts on its width and
three parts on its height, a \emph{Rule of thirds} of the image framing.

For each of these groups of features a feature extractor was built and run over
all images in the dataset.

\subsection{Kolmogorov complexity}

To estimate the Kolmogorov complexity of each channel we follow Romero's
procedure \cite{jma12clas} in finding the ratio between the file size of
uncompressed and compressed, using lossy compression, images.  The PGM format,
in which all channels of the image are stored, is an uncompressed format.  To
produce compressed images we use \emph{JPEG compression}, which is a lossy
compression format.

Following Romero in \cite{jma12clas} we produce the estimates of complexity not
only for each channel but for the edges that can be found in the channel.  We
use two edge detector filters: the \emph{Sobel} filter and the \emph{Canny}
filter, both provided by the \texttt{skimage.filter} python library.  For each
channel we produce a Sobel filtered uncompressed channel (in PGM format) and a
Canny filtered uncompressed channel (also in PGM format).  This totals for 18
PGM images for each image in the dataset.

Next, we compress each of the 18 images with JPEG compression with different
quality values.  We use the \emph{JPEG quality} of 60, 40 and 20, following the
same quality values used by Romero \cite{jma12clas}.  This produces 3 JPEG
images for each of the 18 PGM images, giving a total of 54 JPEG compressed
images for each image in the dataset.

The Kolmogorov complexity is estimated by dividing the size (in bytes) of the
PGM image by each of it's correspondent JPEG compressed images.  This procedure
generates \emph{54 features} for each image in the dataset.

% We did not use any form of fractal compression.

\subsection{Grey Level Co-occurrence Matrix}

GLCM is a well established technique to produce features based on the texture
of an image, used by Machajdik \cite{mach10clas} and Zirnhelt
\cite{zirnhelt07art} for this exact purpose.  Using scripts that call the
subroutines from the \texttt{skimage.feature} python library we produce three
GLCMs for each channel: using the neighbouring pixel on the right of each
pixel, using the 6th pixel to the right and using 20th pixel to the right.  For
each of these GLCMs we calculate the contrast, the energy, the homogeneity and
the correlation of the texture.  Correlation is not always possible to
calculate, in such cases we consider the texture correlation as 1.0.  The
texture features using the GLCM result in a total of \emph{72 features} (6
channels times 3 GLCMs times 4 features).

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.30\textwidth]{L_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{sobel_L_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{canny_L_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{L_rembrandt_eu_464}
\includegraphics[width=0.30\textwidth]{sobel_L_rembrandt_eu_464}
\includegraphics[width=0.30\textwidth]{canny_L_rembrandt_eu_464}
\caption[Canny and Sobel filters of Lightness]{Canny and Sobel filters of the
Lightness channel of an image.  For both images, to the left is the lightness
channel, in the centre is the \emph{Sobel filter} applied to this channel and
to the right is the \emph{Canny filter} applied to this channel.}
\label{fig:cannysobell}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.30\textwidth]{CS_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{sobel_CS_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{canny_CS_caravaggio_1962_139_1}
\includegraphics[width=0.30\textwidth]{CS_rembrandt_eu_464}
\includegraphics[width=0.30\textwidth]{sobel_CS_rembrandt_eu_464}
\includegraphics[width=0.30\textwidth]{canny_CS_rembrandt_eu_464}
\caption[Canny and Sobel filters of Colourfulness]{Canny and Sobel filters of
Colourfulness.  Again, to the left is the original image, in the centre the
\emph{Sobel filter} is applied and to the right the \emph{Canny filter} is
applied.}
\label{fig:cannysobelcs}
\end{figure*}

\subsection{Image Averages}

A simple set of features is the \emph{pixel value average} of each channel of
the image, and the \emph{standard deviation of the value of each pixel} of each
channel.  Both Romero \cite{jma12clas} and Machajdik \cite{mach10clas} use
these measures as features.  The average and standard deviation of each channel
produces \emph{12 features} for each image.

\subsection{Rule of Thirds}

Enhancing the basic image averages Machajdik \cite{mach10clas} uses features of
the framing of the image.  The image is divided in three parts on width and
height to produce \emph{nine rectangles of equal sizes}.  The value of the
average of the value of each pixel and the standard deviation of the value of
each pixel from the central rectangle only is taken as a feature.  These are
calculated for the central rectangle on each of the channels of the image,
resulting in \emph{12 features} per image.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.48\textwidth]{r3_L_caravaggio_1962_139_1}
\includegraphics[width=0.48\textwidth]{r7_L_caravaggio_1962_139_1}
\caption[Rule of thirds]{Rule of \emph{thirds} consist of dividing the image
into three equal parts on each axis, as seen on the image on the left.  It is
common that the colours and objects in the central region are the most
important for the scene painted.  A rule of \emph{sevenths}, figure on the
right, was also used in the segmentation process.  Dividing into seven equal
parts on each axis produce smaller regions than the rule of thirds but big
enough to be noticeable for the contrast of the image.}
\label{fig:r3}
\end{figure*}

\section{Itten Colours}

Johannes Itten cited in \cite{mach10clas} studied colours int the context of
art and their emotional effects on a person looking at a piece of art.  He
defined several \emph{contrasts and combination of colours} that produced
harmonious or other emotional effects when looked upon.  Itten used Hue,
Saturation and Luminance to describe colours in his study, yet Machajdik
\cite{mach10clas} simplified some of the work to use Hue, Saturation and
Lightness.

Features derived from the Itten colours are used in Affective Image retrieval
\cite{mach10clas}.  Yet art is closely related to the affective effect of
images, therefore it does make sense to use affective features to distinguish
art.  We use aggregation of colours to \emph{Itten colours} as features of the
image, and we also use several of the contrasts defined by Itten: contrast of
\emph{Saturation}, contrast of \emph{Light and Dark}, contrast of \emph{Hue},
contrast of \emph{Warm and Cold}, contrast of \emph{Complements}.

\subsection{Amount of each colour}

There are 12 Itten colours.  Considering the hue channel as a circle
with $360^{\circ}$ we can say that each $30^{\circ}$ we have an Itten colour,
starting from the red colour at $0^{\circ}$.  The Itten colours are therefore:
\emph{red}    ($0^{\circ}$),   \emph{orange red}    ($30^{\circ}$),
\emph{orange} ($60^{\circ}$),  \emph{orange yellow} ($90^{\circ}$),
\emph{yellow} ($120^{\circ}$), \emph{green yellow}  ($150^{\circ}$),
\emph{green}  ($180^{\circ}$), \emph{green blue}    ($210^{\circ}$),
\emph{blue}   ($240^{\circ}$), \emph{purple blue}   ($270^{\circ}$),
\emph{purple} ($300^{\circ}$) and \emph{purple red} ($330^{\circ}$).

To get the amount of each colour we count the number of pixels $15^{\circ}$ in
each direction (clockwise and counterclockwise) from the Itten colour.  We
scale the \emph{hue channel} of the image to the range 0 to 360 and add 15 to
all pixels and take modulus 360 from the result, this ensures that all pixels
$15^{\circ}$ from the \emph{red colour} are in the range 0 to 30.  Then we
construct a histogram with 12 bins, spamming $30^{\circ}$ each, from all pixels
in the hue channel.

To generate the features we take only the most prominent colours.  From each of
the 12 bins we subtract the average of all the bins, then every bin with a
positive value is taken as a feature and every bin with a value of zero or a
negative value is left and a 0 is written out as the feature value for that
colour.  A total of \emph{12 features} is generated for each image, one for
each Itten colour.

\subsection{Segmentation}

To measure contrasts (Itten contrasts) we need two different areas of the image
to measure the contrast between them.  i.e. we need to divide the image into
different areas, it is know as \emph{segmenting the image}.  A simple way of
segmenting the image is to use Machajdik's rule of thirds \cite{mach10clas} and
consider each of the nine rectangles as areas we calculate contrast between.
We, therefore, use the \emph{rule of thirds} as one of the segmentations of the
image.  Yet the rule of thirds can be extended to a \emph{rule of sevenths} in
which the image is divided into seven pieces on its width nd height producing
49 equal size rectangles, we use this rule as another segmentation technique.

A more elaborate segmentation of the image is the \emph{watershed
segmentation}, in which we divide the image based on the continuous regions and
the edges between the regions.  The \texttt{skimage.morph} python library was
used to perform watershed segmentation on the channels of the images.
Watershed segmentation produces several regions of different sizes and shapes
over the image channel.  Several of the regions are very small and irrelevant
to a contrast that shall be \emph{perceived by the human eye}, we eliminate
these small regions by removing from the result of the watershed segmentation
all regions smaller than 548 pixels (548 is the square root of 300 000, which
is the number of pixels we normalised all images to).  An example of watershed
segmentation is shown in Figure \ref{fig:segm}.

Segmented regions are assumed to be homogeneous and have the value of the mean
of all pixels of the region.  The regions segmented have the same sizes in the
rule of thirds and rule of sevenths techniques, but in the watershed
segmentation the resulting regions are of \emph{different sizes}.  To make for
the difference in sizes the contrast value calculated from a region is scaled
by the region's size.

To calculate each of the Itten contrasts across the regions resulting from
either the segmentation by rule of thirds, rule of sevenths or watershed
segmentation, we apply a contrast function between \emph{all regions} or
between all \emph{combinations of two regions} (e.g. a total of $9^2$
combinations for the rule of thirds) depending of the type of contrast.  The
result of applying the contrast function to each combination produces a set of
contrast results which need to be processed further.  The scaling by the size
of the region used when calculating the contrast is done on \emph{individual
regions} when the contrast is calculated over all regions, or is done on the
result of the contrast function applied to two regions and scaled on the
\emph{sum of both regions size}.

A contrast over all regions result in a single value, the standard deviation
between all regions, and is taken as a feature.  But contrast between pairs of
regions result in a \emph{set of contrasts}, these are evaluated using a
threshold or a maximum function depending on the Itten contrast to produxe a
feature value.

\begin{sidewaysfigure*}[p]
\centering
\includegraphics[width=0.9\textwidth]{segm_rembrandt_eu_464}
\caption[Watershed segmentation example]{Watershed segmentation example.  The
images at the top show the steps for the segmentation of the image.  The
original image, a denoised image on which the segmentation is actually made,
the gradient magnitude, and the original places of the markers.  Next the
markers \emph{flow as water} filling the regions, this is shown on the next
image.  Finally, the last image in the first row shows how the found regions
are distributed over the original image.  The second row show the five biggest
regions found as a mask over the image.  The last row show the mask generated
by each of the five biggest regions applied to the image and cropped.}
\label{fig:segm}
\end{sidewaysfigure*}

\subsection{Itten Contrasts}

Each Itten contrast is evaluated in each of the three segmentations, and over
two channels of the original image.  We perform the contrast of \emph{Light and
Dark} over the value and lightness channels, the contras of \emph{Saturation}
over the HSV saturation channel and over the HSL saturation channel, and
perform the contrasts of \emph{Hue}, \emph{Warm and Cold} and
\emph{Complements} over the hue and colourfulness channels.  In total we
extract \emph{30 features} (3 segmentations times 5 contrasts times 2
channels).

Contrast of \emph{Light and Dark} is applied to the \emph{Lightness} channel
and to the \emph{Value} channel. The value of the contrast is the standard
deviation between the homogeneous regions in the channel (the contribution of
each region is also scaled by the regions size).

Contrast of \emph{Saturation} is applied to the \emph{Saturation} channel,
separately to the saturation from the HSV colour space and to the saturation
from the HSL colour space.  Just like the contrast of Light and Dark, the value
of the contrast of Saturation is the standard deviation between the regions.

Contrast of \emph{Hue} is applied to the \emph{Hue} channel and also to the
\emph{Colourfulness} channel as it is a Hue damped by Saturation.  It is again
similar to the contrast of Light and Dark and to the contrast of Saturation as
it takes the standard deviation between the regions.  Yet, the Hue and
Colourfulness channels are scaled to the range of 0 to 360 before calculating
the contrast (instead of the range 0 to 255, which is used in Light and Dark
and Saturation contrasts).

\begin{figure*}[!htb]
\begin{equation}
\begin{aligned}
n_1       &= max(0.1, 1 - warm_1 - cold_1) \\
n_2       &= max(0.1, 1 - warm_2 - cold_2) \\
contrast  &= \frac{ max\left( \left\lvert \frac{warm_1}{warm_2}
                                        - \frac{cold_1}{cold_2} \right\rvert
                            , \left\lvert \frac{warm_2}{warm_1}
                                        - \frac{cold_2}{cold_1} \right\rvert
                       \right)
                 }{ n_1 \times n_2 }
\label{eq:coldwarm}
\end{aligned}
\end{equation}
\end{figure*}

\begin{figure*}[!htb]
\begin{equation}
\begin{aligned}
n_1       &= 1 - warm_1 - cold_1 \\
n_2       &= 1 - warm_2 - cold_2 \\
contrast  &= \frac{ warm_1 \times warm_2
                   + n_1    \times n_2
                   + cold_1 \times cold_2
                  }{ \sqrt{      (warm_1^2 + n_1^2 + cold_1^2)
                          \times (warm_2^2 + n_2^2 + cold_2^2) } }
\label{eq:mach}
\end{aligned}
\end{equation}
\end{figure*}

\begin{figure*}[!htb]
\begin{equation}
D = min( \lvert H_1 - H_2 \rvert , 360 - \lvert H_1 - H_2 \rvert )
\label{eq:wheel}
\end{equation}
\end{figure*}

Contrast of \emph{Warm and Cold} is a combination of the ratio of warm colours
(\emph{warmth}), ratio cold colours (\emph{coldness}) and a value of colour
\emph{neutrality} between two regions.  Warmth of a region is the number of
pixels of the region containing warm colours divided by the total number of
pixels of the region, coldness is the total number of pixels of the region
containing cold colours divided by the total number of pixels of the region and
colour neutrality is 1 minus warmth and coldness.  Therefore values for warmth,
coldness and neutrality are always between 0 and 1 for any given region.

\emph{Warm colours} and \emph{Cold colours} are defined as part of the Itten
colours.  Warm colours are: red, range red, orange and orange yellow; cold
colours are: green, green blue, blue and purple blue.  Not that the Itten
colours yellow, green yellow, purple and purple red are not included in neither
warm or cold colours.

\begin{table*}[pht]  % table* instead of table because of multicols
\centering
\rowcolors{2}{}{lightgray}
\begin{tabular}{l|r|r|r|r|r|r}
\toprule
\multirow{2}{*}{Test ID}
  & \multicolumn{2}{c}{region 1}
  & \multicolumn{2}{c}{region 2}
  & \multirow{2}{*}{Eq. \ref{eq:mach}}
  & \multirow{2}{*}{Eq. \ref{eq:coldwarm}}
  \\
\cline{2-5}
& warmth & coldness & warmth & coldness \\
\midrule
1 & 0.5 & 0.5 & 0.3 & 0.3 & 1.00 & 0.00  \\
2 & 0.7 & 0.1 & 0.1 & 0.3 & 0.28 & 55.56 \\
3 & 0.6 & 0.1 & 0.1 & 0.3 & 0.39 & 31.48 \\
4 & 0.5 & 0.1 & 0.1 & 0.3 & 0.55 & 19.44 \\
5 & 0.1 & 0.1 & 0.1 & 0.3 & 1.23 & 4.17  \\
6 & 0.7 & 0.1 & 0.1 & 0.5 & 0.34 & 85.00 \\
7 & 0.5 & 0.1 & 0.1 & 0.7 & 0.59 & 85.00 \\
\bottomrule
\end{tabular}
\caption[Contrast of Warm and Cold]{Contrast of Warm and Cold evaluated by
Equations \ref{eq:mach} and \ref{eq:coldwarm}.  From test 1 we see that
Equation \ref{eq:mach} results in a value for contrast different than zero when
the amount of warmth and coldness is exactly the same for both regions, this
does not happen when using Equation \ref{eq:coldwarm}.  From tests 2, 3 and 4
we see that Equation \ref{eq:mach} is not consistent when only one parameter
change, although the difference between the warmths of the regions reduces the
contrast value produced increases.  Test 5, 6 and 7 show that Equation
\ref{eq:coldwarm} produces high values for contrast when regions are very
different and small values for contrast when regions are similar.}
\label{tab:cweq}
\end{table*}

Machajdik \cite{mach10clas} proposes an equation for the contrast of Warm and
Cold that results in a high value for contrast when the \emph{amount of warm an
cold colours vary} between the regions contrasted, Equation \ref{eq:mach} show
Machajidik's approach to the contrast of Warm and Cold.  Yet, Machajidik's
equation performs poorly as a measure of contrast because of it's inconsistency
when only one of the four factors (warmth of region 1, coldness of region 1,
warmth of region 2, coldness of region 2) changes when the other three factors
remain the same.  To work around this issue we developed a \emph{different
equation} for the contrast of Warm and Cold, Equation \ref{eq:coldwarm}.  The
contrast by this equation scales up when one region is highly warm and the
other highly cold, also there is a hard limit for the neutrality of a region
which cannot be come less than 0.1.  A simple comparison between the two
equations can be seen in Table \ref{tab:cweq}.

Finally, we apply Equation \ref{eq:coldwarm} between all combinations of
regions to retrieve a set of \emph{contrast values}.  The value for the feature
of contrast of Warm and Cold is the \emph{maximum} contrast value from that
set.  Contrast of Warm and Cold is applied to the \emph{Hue} channel and to the
\emph{Colourfulness} channel, similar to the contrast of Hue.

Contrast of \emph{Complements} is also applied to the \emph{Hue} channel and
the \emph{Colourfulness} channel, it searches for contrast between
complementary colours.  \emph{Complementary colours} are colour on opposite
side of the Hue wheel.  We scale the channel to the range 0 to 360 and apply
the contrast function described in Equation \ref{eq:wheel} between all
combinations of two regions resulting is a set of contrasts.  Equation
\ref{eq:wheel} account for the \emph{hue wheel problem} and finds the closest
distance between the colours of two regions, each resulting contrast in the
contrast set is in th range 0 to 180.

For segmentation with the rule of thirds and rule of sevenths we take the value
of the contrast of Complements to be the \emph{number of contrasts from the
contrast set above a threshold}.  The threshold is 160 for the Hue channel and
125 for the Colourfulness channel (as this channel is damped).  On the
watershed segmentation we take the contrast of Complements to be the
\emph{maximum contrast value} in the set (remember that in the watershed
segmentation the value of the contrast is scaled by the size of the regions,
therefore a fixed threshold would not work for any image).

For reference, all features are named and numbered in Table \ref{tab:glossary}
in Appendix \ref{chap:glossary}.

% We did not do the contrast of extension, as it was not described.  Also we
% did not perform the Itten Harmony as it is just the absence of contrast of
% complements.

\begin{table*}[htp]
\centering
\begin{tabular}{|l|l|}
\toprule
Art Movement & Artists (number of paintings) \\
\bottomrule \toprule
\multirow{2}{*}{Renaissance (126)}
                     &  Bassano (17), Botticelli (6), Raphael (52), \\
                     &  Titian (34), Veronese (17)                  \\
\midrule
\multirow{7}{*}{Baroque (521)}
                     &  Berchem (47), Caravaggio (8), Cuyp (20),      \\
                     &  Dujardin (14), Greco (4), Hondecoeter (10),   \\
                     &  Miereveld (12), Molenaer (12), Monnoyer (19), \\
                     &  Murillo (38), Poussin (20), Rebecca (14),     \\
                     &  Rembrandt (30), Ribera (28), Ricci (25),      \\
                     &  Rosa (31), Rubens (61), Teniers (76),         \\
                     &  Vel\'azquez (24), Wouwerman (28)              \\
\midrule
\multirow{2}{*}{Neoclassicism (195)}
                     &  Canaletto (34), Carlevariis (51), Dughet (30), \\
                     &  Kauffmann (22), Panini (38), Zoffany (20)      \\
\midrule
\multirow{5}{*}{Romanticism (215)}
                     &  Boudin (26), Constable (39), Daubigny (13),  \\
                     &  Delacroix (9), Diaz (17), Gainsborough (13), \\
                     &  Goya (8), Latour (31), Leslie (10),          \\
                     &  Loutherbourg (17), Mulready (10),            \\
                     &  Rousseau (12),  Watts (10)                   \\
\midrule
Impressionism (32)   &  Gauguin (4), Monticelli (18), Pissarro (10) \\
\bottomrule \toprule
Watercolour (16)     &  Carpenter (9), Tagore (7) \\
\midrule
\multirow{2}{*}{Mughal (79)}
                     &  Basawan (11), Jagan (9), Kesav (15), La'l (19), \\
                     &  Miskin (15), Tulsi (10)                         \\
\midrule
N/A                  &  Devi (16) \\
\bottomrule
\end{tabular}
\caption[Artists in art movements]{Artists in art movements, or art schools.
Several simplifications were made to allow for a smaller number of classes.
The top of the table represents a simplified version of the \emph{history of
art in Europe}, starting with \emph{Renaissance} in the 15th-16th centuries,
\emph{Baroque} in the 17th century, \emph{Neoclassicism} in the 18th century,
\emph{Romanticism} containing all styles of the early 19th century and
\emph{Impressionism} including all styles of the late 19th century.  The bottom
of the table includes movements unrelated to the mentioned ones (\emph{Mughal
miniature painting style}) or using different materials (\emph{Watercolour}).}
\label{tab:style}
\end{table*}

\section{Extra Metadata}

For the extrapolation of artist classification into \emph{art school} (or art
movement) extra metadata is needed.  Each image must link to an art school
apart from linking to an artist.  To make this possible we classified each
artist into a specific art school.  This is a big simplification of the reality
as artists moved between art schools/movements during their lifetimes, yet the
assignment was made based on the majority of the work by the artist.  Most of
the information needed to assign an art school to an artist could be collected
directly from the VAM and NIRP collections, yet it was a manual process.

Art classification into schools or movements is always \emph{subjective}
\cite{dimaggio87art}, therefore VAM classification and NIRP classification do
not match each other.  We added several simplifications to make the assignment
of an art school to an artist easier and to make the number of art schools in
the dataset smaller.  The general \emph{Italian Renaissance} and the
\emph{Venetian school} were both put under the same banner: \emph{Renaissance}.
\emph{Mannerism}, the \emph{Dutch golden age} of paintings were both put under
the banner of \emph{Baroque}.  \emph{Realism}, \emph{Symbolism}, the
\emph{Barbizon school} were all put under \emph{Romanticism}.  And both
\emph{Post-Impressionism} and \emph{Neo-Impressionism} were assigned simply to
\emph{Impressionism}.  Table \ref{tab:style} shows all art schools and the
artists assigned to them after all simplifications.

With each artist assigned to an art school, we added to the extracted features
the metadata containing the artist's surname, the art school and the country in
which the artist was active.  The country is not used in this work, yet it is
an interesting piece of metadata that might be used in future work.

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.29\textwidth]{basawan_2009BX3686}
\includegraphics[width=0.29\textwidth]{titian_eu_741}
\includegraphics[width=0.29\textwidth]{murillo_1974p24}
\includegraphics[width=0.33\textwidth]{kauffmann_va_pc_2006an1773}
\includegraphics[width=0.33\textwidth]{loutherbourg_k264}
\includegraphics[width=0.26\textwidth]{monticelli_300}
\caption[Examples of paintings in art movements/schools]{Examples of paintings
in the movements/schools used.  Upper left is an example of Mughal miniature
paintings, it is the painting "Mu'nim Khan and Khwaja Jahan" by \emph{Basawan},
it is the only example of non-european style.  All following paintings in the
example depict the historical evolution of art movements in Europe: at the top
in the centre is the \emph{Renaissance} period painting "Virgin and Child with
Saint Catherine" by \emph{Titian};  top right it the \emph{Baroque} painting
"Vision of St Anthony of Padua" by \emph{Bartolom\'e Esteb\`an Murillo};  in
the middle to the left is the painting from the \emph{Neoclassicism} school "A
Sleeping Nymph Watched by a Shepherd" by \emph{Angelica Kauffmann}; in the
middle to the right is the \emph{Romantic} painting "The Cutting Out of the
French Corvette 'La Chevrette'" by \emph{Philip James de Loutherbourg}; and at
the bottom is the \emph{Impressionist} painting "The Echo in the Woods" by
\emph{Adolphe Joseph Thomas Monticelli}.}
\label{fig:styles}
\end{figure*}

\end{multicols}

%Citations useful for the introduction (and for the remaining of the text too):
%Romero \cite{jma12clas}, Romero \cite{rmc12ajs}, Chen (Yahoo) on JSONS
%\cite{chen09yahoo}, Zirnhelt (generic artist classification)
%\cite{zirnhelt07art}, Machajdik (affective) \cite{mach10clas}, Chih-Wei
%(libsvm) \cite{hcl03svm}, DiMaggio (art classification) \cite{dimaggio87art}

\chapter{Results}
\label{chap:results}
\begin{multicols}{2}

% No PCA was done, we can say that the features are unrelated enough that we do
% not need feature selection.

% PCA between pairs of authors, maybe?

% Pearson coefficient was not used, we do not have any categorical data instead
% all data for correlation is numeric.

% Extrapolation towards, art movement/school is the most important part of this
% chapter.

\section{Correlation between features}

Artists in the same art movement shall be close, in distinct movements shall be
apart.

The absolute value of the correlation coefficient on the scatter graph shall be
good enough.  It can be seen on the graph that few correlation coefficients are
above 0.7.

\section{Classification}

How we calculated the correlation.  Most significant numbers from each table.
A plethora of tables.  A graph with three most prominent features between each
school.

%\section{meeting: discussion on CBIR (Friday, 1st of July)}
%
%Amount of noise (by random sample) in the cleansed datasets.
%
%Start writing about dataset cleansing heuristics, plus some *descriptions* and
%*statistics*.
%
%Get some of the artists (with most paintings) and do some statistics over
%those.
%
%Histograms of features (especially itten colours) by different artists.
%
%R library to explore data: ggplot2 (use it?).
%

\begin{table*}[ptb]
\centering
\rowcolors{1}{}{lightgray}
\begin{tabular}{|l|r||l|r||l|r|}
\toprule
artist & acc. & artist & acc. & artist & acc. \\
\midrule
\input{art-predict.tex}
\bottomrule
\end{tabular}
\caption[Multiclass artist classification]{Multiclass artist classification}
\label{tab:predart}
\end{table*}

\begin{table*}[ptb]
\centering
\begin{tabular}{|l|r||l|r|}
\toprule
school & acc. & school & acc. \\
\midrule
\input{sch-predict.tex}
\bottomrule
\end{tabular}
\caption[Multiclass school classification]{Multiclass school classification}
\label{tab:predsch}
\end{table*}

\section{Cross Classification}

When we take two classes only and run the classifiers between them the results
are better.  This is because classification between two classes is easier as
classifiers can concentrate on specific points that are different between those
two classes.

\begin{table*}[ptb]
\centering
\begin{tabular}{r||r|r|r|r|r}
\toprule
\input{art-compm.tex}
\bottomrule
\end{tabular}
\caption[Two class artist classification]{Two class artist classification}
\label{tab:foldart}
\end{table*}

\begin{table*}[ptb]
\centering
\rowcolors{5}{}{lightgray}
\begin{tabular}{r||r|r|r|r|r}
\toprule
\input{sch-compm.tex}
\bottomrule
\end{tabular}
\caption[Two class school classification]{Two class school classification}
\label{tab:foldsch}
\end{table*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=\textwidth]{corr-graph}
\caption[Correlation]{Correlation Coefficient}
\label{fig:corr}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=\textwidth]{school-graph}
\caption[Feature Values]{Feature values per School}
\label{fig:school}
\end{figure*}

\begin{figure*}[tbp]
\centering
\includegraphics[width=0.7\textwidth]{class-graph}
\caption[Classification]{Classification Accuracy}
\label{fig:class}
\end{figure*}

\end{multicols}

\chapter{Conclusions}
\begin{multicols}{2}

One of the reasons for the small number of works in the field of classifying
art by the \emph{style} or \emph{aesthetics} could possibly be the difficulty
in finding big and consistent datasets of visual art.  Whilst on photography
there are several photo sharing websites e.g. \texttt{panoramio.com} or
\texttt{flickr.com}, with powerful APIs, consistent sets of metadata and
millions of images;  a consistent datasets of thousands of art images is hard
to acquire.  The dataset we created for this work is made of images in public
collections only, and the Copyright of the images is permissible given the
reduced size and number of images.  All effort is being made to ensure the
openness of Copyright rules for the dataset, and it shall then be published.

The dataset and the extracted features being available openly allow future
works to compare against this work, as several parts of this work can be
improved.  A \emph{feature selection} process will certainly improve the
classification, and allow for a better understanding of the most important
feature for paintings among the features described.

\end{multicols}

\newpage
\phantomsection
\addcontentsline{toc}{chapter}{\bibname}
\bibliographystyle{plain}
\bibliography{capybara}

\appendix

\chapter{Requirements to run the code}
\label{chap:requirements}

Describe each library installed, there are some.

Compiled software (often available as software packages as .rpm or .deb)
\begin{itemize}
\item[]\emph{ImageMagick} 6.5.4-7 or later, \\
available at \href{http://www.imagemagick.org/}{http://www.imagemagick.org/}

\item[]\emph{libsvm} 3.18 or later, \\
available at \href{http://csie.ntu.edu.tw/~cjlin/libsvm/index}
{http://csie.ntu.edu.tw/~cjlin/libsvm/index}
\end{itemize}

Provided by LSB:
\begin{itemize}
\item[]\emph{python} 2.6.6 or later (but not python 3), \\
available at \href{http://www.python.org/}{http://www.python.org/}

\item[]\emph{perl} 5.8.8 or later, \\
available at \href{http://www.perl.org/}{http://www.perl.org/}

\item[]\emph{bash} 4.1.0 or later, \\
available at \href{http://www.gnu.org/software/bash/index.html}
{http://www.gnu.org/software/bash/index.html}
\end{itemize}

Python packages:
\begin{itemize}
\item[]\emph{numpy} 1.8.1 or later, \\
available at \href{http://www.numpy.org/}{http://www.numpy.org/}

\item[]\emph{scipy} 0.14.0, \\
available at \href{http://www.scipy.org/}{http://www.scipy.org/}

\item[]\emph{matplotlib} 0.99.1.1, \\
available at \href{http://matplotlib.org/}{http://matplotlib.org/}

\item[]\emph{pillow} 2.5.1 or later, \\
available at the pypi index at
\href{https://pypi.python.org/pypi/Pillow}
{http://pypi.python.org/pypi/Pillow} \\
or directly from github at
\href{http://github.com/python-pillow/Pillow}
{http://github.com/python-pillow/Pillow}

\item[]\emph{scikit-image} 0.10.1, \\
available at \href{http://scikit-image.org/}{http://scikit-image.org/}

\item[]\emph{ipython} 1.0.0 or later, \\
available at \href{http://ipython.org/}{http://ipython.org/}

\item[]\emph{pandas} 0.14.1, \\
available at \href{http://pandas.pydata.org/}{http://pandas.pydata.org/}
\end{itemize}

For dataset acquisition
\begin{itemize}
\item[]\emph{beautifulsoup4} 4.3.2 or later, \\
available at \href{http://www.crummy.com/software/BeautifulSoup/}
{http://www.crummy.com/software/BeautifulSoup/}

\item[]\emph{pyyaml} 3.10 or later, \\
available at \href{http://pyyaml.org/}{http://pyyaml.org/}

\item[]\emph{git} 1.5.0 or later is needed to clone unixjsons from its
repository, available at \href{http://www.git-scm.com/}{http://www.git-scm.com/}

\item[]\emph{unixjsons} 0.7 or later, \\
available at \href{https://github.com/grochmal/unixjsons}
{https://github.com/grochmal/unixjsons}
\end{itemize}

\chapter{How to run the code}
\label{chap:runthecode}

Before running the code make sure you are in an LSB compatible system\footnote{
\href{http://www.linuxfoundation.org/collaborate/workgroups/lsb}
{http://www.linuxfoundation.org/collaborate/workgroups/lsb}} and install the
libraries described in Appendix \ref{chap:requirements}.  Copy the code into a
directory\footnote{If git and an internet connection are available you can
clone the code directly from its repository with
\verb|git clone https://github.com/grochmal/capybara.git|.}
and go to \texttt{src/}.

Acquisition and cleansing of the datasets is not covered because that process
is not automated.  To browse the code used to acquire the datasets go to
\texttt{dataset-acquisition/vam} and \texttt{dataset-acquisition/nirp} for the
VAM and NIRP datasets respectively.

In \texttt{src/} run the scripts that normalise, separate and filter the images
before the extraction of features:

\begin{Verbatim}[frame=leftline]
./step01-normalise.sh      # normalise to ~300 000 pixels
./step02-hsl-hsv.sh        # separates the channels
./step03-colourfulness.sh  # calculates the colourfulness
./step04-sobel.sh          # applies the Sobel filter
./step05-canny.sh          # applies the Canny filter
./step06-jpeg.sh           # compress the images
\end{Verbatim}

Next run the feature extractors.  These scripts print the calculated features
to the \emph{standard output stream} and logging to the \emph{standard error
stream}, therefore redirect the standard output to files:

\begin{Verbatim}[frame=leftline]
./step07-kolmogorov.sh     > fe/kolmogorov.dat
./step08-glcm.sh           > fe/glcm.dat
./step09-itten12.sh        > fe/itten12.dat
./step10-itten-contrast.sh > fe/itten-contrast.dat
./step11-rule-of-3.sh      > fe/rule-of-3.dat
./step12-img-averages.sh   > fe/img-averages.dat
\end{Verbatim}

A last feature script generating the metadata (artist, school and contry) of
the image shall then be run.  This metadata is what we classify against:

\begin{Verbatim}[frame=leftline]
./step13-metadata.sh > fe/meta.dat
\end{Verbatim}

Format the features to a format compatible with LIBSVM and run the classifiers
using the following scripts.  The results of the classifiers are printed to the
file \texttt{cls/output.dat}.

\begin{Verbatim}[frame=leftline]
./step14-libsvm-format.sh
./step15-classify.sh
\end{Verbatim}

The last script combines the results from the classifiers and the full dataset
of features (the horizontal concatenation of all \texttt{.dat} files) to
produce the graphs and tables present in Chapter \ref{chap:results} and
Appendix \ref{chap:glossary}.

\begin{Verbatim}[frame=leftline]
./step16-outputs.sh
\end{Verbatim}

\chapter{unixjsons}
\label{chap:unixjsons}

During the acquisition of the datasets from the VAM and NIRP collections the
metadata for the paintings was stored in plain files containing a JSON object
encoded on each line.  This file format was called JSONS and originates from
the work by Chen in \cite{chen09yahoo}.  To process the metadata in this format
as a stream several tools were developed: \texttt{jgrep}, \texttt{jcut},
\texttt{jsed}, \texttt{jecho} and \texttt{jjoin}.  These tools are available
from the \emph{unixjsons} repository at github, and can be retrieved with
\emph{git} by cloning the repository:

\begin{verbatim}
git clone https://github.com/grochmal/unixjsons.git
\end{verbatim}


\section{jgrep}

An implementation of \texttt{grep} (and \texttt{egrep}) that works on a JSONS
stream.  Instead of a single regular expression to search for in a stream it
uses two separate regular expressions one to match against the \emph{key} in
the JSONS stream and one to match against the \emph{value} of matched keys.
The usage of \emph{jgrep} follows:

% Inside \small Verbatim use 69 characters max to respect the margins
{\small
\begin{Verbatim}[samepage=true]
Usage: jgrep [-n key] [-hVqviIHR] <key re> <value re> [<file> ...]

  -h, --help
        Print this help.

  -V, --version
        Prints the version of the script.

  -q, --quiet, --silent
        Do not print anything, even on error.

  -n key, --line-number=key
        Give the line number in the output, it is added to the
        result under the key specified as the argument.

  -v, --invert-match
        Print lines that do not match re pattern pattern.
\end{Verbatim}
}

\section{jcut}

An implementation of \texttt{cut} that works on a JSONS stream.  Contrary to
the \texttt{cut} command the \texttt{-f} flag receives the names of the
\emph{keys} in the stream, instead of numbers of the fields as in the normal
\texttt{cut}.  The usage of \texttt{jcut} follows:

{\small
\begin{Verbatim}[samepage=true]
Usage: jcut [-chrlV] [-f field,field,...] [<file> ...]

  -h, --help
        Print this help.

  -V, --version
        Prints the version of the script.

  -f, --fields=
        Comma delimited list of fields to cut from the JSON.

  -c, --complement
        Print all fields that are not in the fields list.

  -r, --regex
        Treat every field in the field list as a regex, this
        is useful to pattern match several similar fields or
        to match a field with a comma in it (using \\054).

  -l, --list
        Print all available field in the input, then exit.
\end{Verbatim}
}

\section{jsed}

A simplified implementation of \texttt{sed} command that works on a JSONS
stream.  It implements the \texttt{s///} substitution command for several
combinations of key and value pair combinations.  Both keys and values can be
matched and/or changed using substitution regular expressions.  The usage of
\texttt{jsed}, including some examples follows:

{\small
\begin{verbatim}
Usage: jsed [-hVqiI] [-f file | -e expr -e expr ...] [<file> ...]

  -h, --help
        Print this help.

  -V, --version
        Prints the version of the script.

  -q, --quiet, --silent
        Do not print anything, even on error.

  -e /pattern/replacement/, --expression=/pattern/replacement/
  -e /pattern/replacement/flags
  -e /pattern/pattern/replacement/
  -e /pattern/pattern/replacement/flags
  -e /pattern/replacement/pattern/replacement/
  -e /pattern/replacement/pattern/replacement/flags
  -e {pattern}{replacement}
  -e {pattern}{replacement}flags
  -e {pattern}{pattern}{replacement}
  -e {pattern}{pattern}{replacement}flags
  -e {pattern}{replacement}{pattern}{replacement}
  -e {pattern}{replacement}{pattern}{replacement}flags
        The substitution expression to execute, this expression
        will be run on every line on every key that matches.
        Several expressions can be specified on a single
        invocation and will be executed in the order they're
        present in the command line.  The pattern part of the
        expression is a regular expression, whilst the
        replacement part is a string that accepts '\'
        groupings (e.g. \1 for first grouping).

        An expression is divided into fields, each field is
        delimited by a delimiter character.  The first character
        of the expression becomes the delimiter character, every
        occurrence of that character thereof will mark the start
        of a new field.  The delimiter character must be
        printable and cannot be a digit (0-9) or one of the
        characters: 'g', 'i', '.' or '#'.  The delimiter
        character cannot be used in the fields of the
        expression.

        Also, the three grouping pairs characters '()', '[]'
        and '{}' can be used to group the fields together.
        Note that using '()' will not allow the use of grouping
        in the pattern and using '[]' will not allow the use of
        character definitions.

        Examples of equivalent expressions follows:

        Change the key 'image_id' to 'image':
            -e /image_id/image/
            -e ;image_id;image;
            -e "image_id"image"
            -e {image_id}{image}
            -e (image_id)(image)

        Every image key content from .jpg to .png
            -e /image_.*/\.jpg$/.png/g
            -e =image_.*=\.jpg$=.png=g
            -e [image_.*][\.jpg$][.png]g

        First change every field containing `image` to start
        with this word, then change every occurrence of .jpg
        to .png (note: if two fields are named `first_image`,
        `second_image` the first field will be overridden by
        the second because the substitution happens in
        ASCIIbetical order of fields).
            -e /.*image(.*)/image\1/g -e /image.*/\.jpg/.png/g
            -e {.*image(.*)}{image\1}g -e {image.*}{\.jpg}{.png}g
            -e /.*image(.*)/image\1/g -e {image.*}{\.jpg}{.png}g

        More on the purpose of each field  and the `flags`
        suffix in the -e expression can be found in the
        `Expression` section, below.

  -f script_file, --file=script_file
        The script file contains one expression (as in the -e
        option) per line, which are executed in order as they
        appear in the input.  Every expression in the
        `script_file` is exactly the same as if it was passed
        to the -e flag.

        If both -f and -e are present (possibly several -e
        flags) the changes to each line of the input will
        happen by first executing the script passed by -f
        and then executing all -e flags in order.

  -i, --in-place
        Change the file in place, the default is to print the
        modified file to standard output.  If the input is
        composed of standard input only, of several files or
        of a combination of standard input and files this flag
        is ignored.

  -b extension , --backup=extension
        If -i is in effect copy the input file is copied and the
        extension `extension` is appended to it.  Only then the
        original file is modified.  If -i is in effect and -b is
        not present the changes made are irreversible.
\end{verbatim}
}

\section{jecho}

A combination of the \texttt{echo} and \texttt{cat} commands, implemented to
work on JSONS streams.  \texttt{jecho} can both concatenate streams (as
\texttt{cat}) and add new \emph{keys} and \emph{values} to the stream (as
\texttt{echo}).  The usage of \emph{jecho} follows:

{\small
\begin{Verbatim}[samepage=true]
Usage: jecho [-hV] -f k:v [-f k:v -f k:v ...] [<file> ...]

  -h, --help
        Print this help.

  -V, --version
        Prints the version of the script.

  -q, --quiet, --silent
        Do not print messages.

  -f k:v, --field=k:v
        Add field `k` with value `v` to the stream.

  -r, --replace
        By default fields already in the stream will not be
        replaced, unless this flag is on.
\end{Verbatim}
}

\section{jjoin}

A combination of the \texttt{join} and \texttt{paste} commands that work on
JSONS streams.  It works as \texttt{paste} if the \emph{keys} in the two
streams are all different, but is a certain \emph{key} is found in both streams
the \emph{values} for that key in both streams are joined (this is behaviour
similar to \texttt{join}).  The usage for \texttt{jjoin} follows:

{\small
\begin{Verbatim}[samepage=true]
Usage: jjoin [-hVq] file1 file2

  -h, --help
        Print this help.

  -V, --version
        Prints the version of the script.

  -q, --quiet, --silent
        Do not print messages.

  -, STDIN
        Read from STDIN instead of file.
\end{Verbatim}
}

\chapter{Resources on the CD}
\label{chap:resources}

The CD contains the cleaned datasets and the code to run all experiments under
a directory called \texttt{capybara}.  That directory also contains a README.md
file which mentions the Copyright (Copyleft) of the code and explains how to
run it, in the same way as Appendix \ref{chap:runthecode}.  The CD also
contains a copy of the code of \emph{unixjsons}, which is needed to browse the
intermediate files in acquiring the datasets.  \emph{unixjsons} is mentioned in
Appendix \ref{chap:unixjsons}.

If the CD is not available the code for the experiments can be retrieved using
\emph{git}:

\begin{verbatim}
git clone https://github.com/grochmal/capybara.git
\end{verbatim}

Also, a mentioned in Appendix \ref{chap:unixjsons}, the code for
\emph{unixjsons} can be retrieved using \emph{git} too:

\begin{verbatim}
git clone https://github.com/grochmal/unixjsons.git
\end{verbatim}

\chapter{Feature Values}
\label{chap:glossary}

Table \ref{tab:glossary} present a glossary of all features used, this shall be
used as a reference for reading the legends and graphs.  Tables
\ref{tab:bigtop} and \ref{tab:bigbot} show mean values of each feature
aggregated by the art school.

\begin{center}
\footnotesize
\begin{longtable}{|l|c|l|}
\toprule
\multirow{2}{*}{Feature} & number   & short name         \\
                         & (graphs) & (in the data file) \\
\midrule
\endhead

\midrule
\multicolumn{3}{r}{{Continued on next page}} \\
\cmidrule{2-3}
\endfoot

\bottomrule
\caption[Feature glossary]{Glossary of all features, divided into the feature
groups described.  For each feature the short name used in legends and tables
is shown, as well as the id (number) of the feature used in graphs.}
\label{tab:glossary}
\endlastfoot

\midrule
\multicolumn{3}{c}{Komogorov Complexity for$\ldots$} \\
\midrule
Colourfulness (quality 60)                      &  0 & nofilter-cs60   \\
Colourfulness (quality 40)                      &  1 & nofilter-cs40   \\
Colourfulness (quality 20)                      &  2 & nofilter-cs20   \\
Hue (quality 60)                                &  3 & nofilter-h60    \\
Hue (quality 40)                                &  4 & nofilter-h40    \\
Hue (quality 20)                                &  5 & nofilter-h20    \\
Lightness (quality 60)                          &  6 & nofilter-l60    \\
Lightness (quality 40)                          &  7 & nofilter-l40    \\
Lightness (quality 20)                          &  8 & nofilter-l20    \\
Saturation$_{HSV}$ (quality 60)                 &  9 & nofilter-shsv60 \\
Saturation$_{HSV}$ (quality 40)                 & 10 & nofilter-shsv40 \\
Saturation$_{HSV}$ (quality 20)                 & 11 & nofilter-shsv20 \\
Saturation$_{HSL}$ (quality 60)                 & 12 & nofilter-shsl60 \\
Saturation$_{HSL}$ (quality 40)                 & 13 & nofilter-shsl40 \\
Saturation$_{HSL}$ (quality 20)                 & 14 & nofilter-shsl20 \\
Value (quality 60)                              & 15 & nofilter-v60    \\
Value (quality 40)                              & 16 & nofilter-v40    \\
Value (quality 20)                              & 17 & nofilter-v20    \\
Sobel filter of Colourfulness (quality 60)      & 18 & sobel-cs60      \\
Sobel filter of Colourfulness (quality 40)      & 19 & sobel-cs40      \\
Sobel filter of Colourfulness (quality 20)      & 20 & sobel-cs20      \\
Sobel filter of Hue (quality 60)                & 21 & sobel-h60       \\
Sobel filter of Hue (quality 40)                & 22 & sobel-h40       \\
Sobel filter of Hue (quality 20)                & 23 & sobel-h20       \\
Sobel filter of Lightness (quality 60)          & 24 & sobel-l60       \\
Sobel filter of Lightness (quality 40)          & 25 & sobel-l40       \\
Sobel filter of Lightness (quality 20)          & 26 & sobel-l20       \\
Sobel filter of Saturation$_{HSV}$ (quality 60) & 27 & sobel-shsv60    \\
Sobel filter of Saturation$_{HSV}$ (quality 40) & 28 & sobel-shsv40    \\
Sobel filter of Saturation$_{HSV}$ (quality 20) & 29 & sobel-shsv20    \\
Sobel filter of Saturation$_{HSL}$ (quality 60) & 30 & sobel-shsl60    \\
Sobel filter of Saturation$_{HSL}$ (quality 40) & 31 & sobel-shsl40    \\
Sobel filter of Saturation$_{HSL}$ (quality 20) & 32 & sobel-shsl20    \\
Sobel filter of Value (quality 60)              & 33 & sobel-v60       \\
Sobel filter of Value (quality 40)              & 34 & sobel-v40       \\
Sobel filter of Value (quality 20)              & 35 & sobel-v20       \\
Canny filter of Colourfulness (quality 60)      & 36 & canny-cs60      \\
Canny filter of Colourfulness (quality 40)      & 37 & canny-cs40      \\
Canny filter of Colourfulness (quality 20)      & 38 & canny-cs20      \\
Canny filter of Hue (quality 60)                & 39 & canny-h60       \\
Canny filter of Hue (quality 40)                & 40 & canny-h40       \\
Canny filter of Hue (quality 20)                & 41 & canny-h20       \\
Canny filter of Lightness (quality 60)          & 42 & canny-l60       \\
Canny filter of Lightness (quality 40)          & 43 & canny-l40       \\
Canny filter of Lightness (quality 20)          & 44 & canny-l20       \\
Canny filter of Saturation$_{HSV}$ (quality 60) & 45 & canny-shsv60    \\
Canny filter of Saturation$_{HSV}$ (quality 40) & 46 & canny-shsv40    \\
Canny filter of Saturation$_{HSV}$ (quality 20) & 47 & canny-shsv20    \\
Canny filter of Saturation$_{HSL}$ (quality 60) & 48 & canny-shsl60    \\
Canny filter of Saturation$_{HSL}$ (quality 40) & 49 & canny-shsl40    \\
Canny filter of Saturation$_{HSL}$ (quality 20) & 50 & canny-shsl20    \\
Canny filter of Value (quality 60)              & 51 & canny-v60       \\
Canny filter of Value (quality 40)              & 52 & canny-v40       \\
Canny filter of Value (quality 20)              & 53 & canny-v20       \\

\midrule
\multicolumn{3}{c}{Grey Level Co-occurence Matrix} \\
\midrule
Contrast of Colourfulness (each pixel)             &  54 & glcm-cs-ct1    \\
Contrast of Colourfulness (each 6 pixels)          &  55 & glcm-cs-ct6    \\
Contrast of Colourfulness (each 20 pixels)         &  56 & glcm-cs-ct20   \\
Energy of Colourfulness (each pixel)               &  57 & glcm-cs-en1    \\
Energy of Colourfulness (each 6 pixels)            &  58 & glcm-cs-en6    \\
Energy of Colourfulness (each 20 pixels)           &  59 & glcm-cs-en20   \\
Homogeneity of Colourfulness (each pixel)          &  60 & glcm-cs-hm1    \\
Homogeneity of Colourfulness (each 6 pixels)       &  61 & glcm-cs-hm6    \\
Homogeneity of Colourfulness (each 20 pixels)      &  62 & glcm-cs-hm20   \\
Correlation of Colourfulness (each pixel)          &  63 & glcm-cs-cr1    \\
Correlation of Colourfulness (each 6 pixels)       &  64 & glcm-cs-cr6    \\
Correlation of Colourfulness (each 20 pixels)      &  65 & glcm-cs-cr20   \\
Contrast of Hue (each pixel)                       &  66 & glcm-h-ct1     \\
Contrast of Hue (each 6 pixels)                    &  67 & glcm-h-ct6     \\
Contrast of Hue (each 20 pixels)                   &  68 & glcm-h-ct20    \\
Energy of Hue (each pixel)                         &  69 & glcm-h-en1     \\
Energy of Hue (each 6 pixels)                      &  70 & glcm-h-en6     \\
Energy of Hue (each 20 pixels)                     &  71 & glcm-h-en20    \\
Homogeneity of Hue (each pixel)                    &  72 & glcm-h-hm1     \\
Homogeneity of Hue (each 6 pixels)                 &  73 & glcm-h-hm6     \\
Homogeneity of Hue (each 20 pixels)                &  74 & glcm-h-hm20    \\
Correlation of Hue (each pixel)                    &  75 & glcm-h-cr1     \\
Correlation of Hue (each 6 pixels)                 &  76 & glcm-h-cr6     \\
Correlation of Hue (each 20 pixels)                &  77 & glcm-h-cr20    \\
Contrast of Lightness (each pixel)                 &  78 & glcm-l-ct1     \\
Contrast of Lightness (each 6 pixels)              &  79 & glcm-l-ct6     \\
Contrast of Lightness (each 20 pixels)             &  80 & glcm-l-ct20    \\
Energy of Lightness (each pixel)                   &  81 & glcm-l-en1     \\
Energy of Lightness (each 6 pixels)                &  82 & glcm-l-en6     \\
Energy of Lightness (each 20 pixels)               &  83 & glcm-l-en20    \\
Homogeneity of Lightness (each pixel)              &  84 & glcm-l-hm1     \\
Homogeneity of Lightness (each 6 pixels)           &  85 & glcm-l-hm6     \\
Homogeneity of Lightness (each 20 pixels)          &  86 & glcm-l-hm20    \\
Correlation of Lightness (each pixel)              &  87 & glcm-l-cr1     \\
Correlation of Lightness (each 6 pixels)           &  88 & glcm-l-cr6     \\
Correlation of Lightness (each 20 pixels)          &  89 & glcm-l-cr20    \\
Contrast of Saturation$_{HSL}$ (each pixel)        &  90 & glcm-shsl-ct1  \\
Contrast of Saturation$_{HSL}$ (each 6 pixels)     &  91 & glcm-shsl-ct6  \\
Contrast of Saturation$_{HSL}$ (each 20 pixels)    &  92 & glcm-shsl-ct20 \\
Energy of Saturation$_{HSL}$ (each pixel)          &  93 & glcm-shsl-en1  \\
Energy of Saturation$_{HSL}$ (each 6 pixels)       &  94 & glcm-shsl-en6  \\
Energy of Saturation$_{HSL}$ (each 20 pixels)      &  95 & glcm-shsl-en20 \\
Homogeneity of Saturation$_{HSL}$ (each pixel)     &  96 & glcm-shsl-hm1  \\
Homogeneity of Saturation$_{HSL}$ (each 6 pixels)  &  97 & glcm-shsl-hm6  \\
Homogeneity of Saturation$_{HSL}$ (each 20 pixels) &  98 & glcm-shsl-hm20 \\
Correlation of Saturation$_{HSL}$ (each pixel)     &  99 & glcm-shsl-cr1  \\
Correlation of Saturation$_{HSL}$ (each 6 pixels)  & 100 & glcm-shsl-cr6  \\
Correlation of Saturation$_{HSL}$ (each 20 pixels) & 101 & glcm-shsl-cr20 \\
Contrast of Saturation$_{HSV}$ (each pixel)        & 102 & glcm-shsv-ct1  \\
Contrast of Saturation$_{HSV}$ (each 6 pixels)     & 103 & glcm-shsv-ct6  \\
Contrast of Saturation$_{HSV}$ (each 20 pixels)    & 104 & glcm-shsv-ct20 \\
Energy of Saturation$_{HSV}$ (each pixel)          & 105 & glcm-shsv-en1  \\
Energy of Saturation$_{HSV}$ (each 6 pixels)       & 106 & glcm-shsv-en6  \\
Energy of Saturation$_{HSV}$ (each 20 pixels)      & 107 & glcm-shsv-en20 \\
Homogeneity of Saturation$_{HSV}$ (each pixel)     & 108 & glcm-shsv-hm1  \\
Homogeneity of Saturation$_{HSV}$ (each 6 pixels)  & 109 & glcm-shsv-hm6  \\
Homogeneity of Saturation$_{HSV}$ (each 20 pixels) & 110 & glcm-shsv-hm20 \\
Correlation of Saturation$_{HSV}$ (each pixel)     & 111 & glcm-shsv-cr1  \\
Correlation of Saturation$_{HSV}$ (each 6 pixels)  & 112 & glcm-shsv-cr6  \\
Correlation of Saturation$_{HSV}$ (each 20 pixels) & 113 & glcm-shsv-cr20 \\
Contrast of Value (each pixel)                     & 114 & glcm-v-ct1     \\
Contrast of Value (each 6 pixels)                  & 115 & glcm-v-ct6     \\
Contrast of Value (each 20 pixels)                 & 116 & glcm-v-ct20    \\
Energy of Value (each pixel)                       & 117 & glcm-v-en1     \\
Energy of Value (each 6 pixels)                    & 118 & glcm-v-en6     \\
Energy of Value (each 20 pixels)                   & 119 & glcm-v-en20    \\
Homogeneity of Value (each pixel)                  & 120 & glcm-v-hm1     \\
Homogeneity of Value (each 6 pixels)               & 121 & glcm-v-hm6     \\
Homogeneity of Value (each 20 pixels)              & 122 & glcm-v-hm20    \\
Correlation of Value (each pixel)                  & 123 & glcm-v-cr1     \\
Correlation of Value (each 6 pixels)               & 124 & glcm-v-cr6     \\
Correlation of Value (each 20 pixels)              & 125 & glcm-v-cr20    \\

\midrule
\multicolumn{3}{c}{Itten Colours} \\
\midrule
Amount of Red           & 126 & it12-r  \\
Amount of Orange Red    & 127 & it12-or \\
Amount of Orange        & 128 & it12-o  \\
Amount of Orange Yellow & 129 & it12-oy \\
Amount of Yellow        & 130 & it12-y  \\
Amount of Green Yellow  & 131 & it12-gy \\
Amount of Green         & 132 & it12-g  \\
Amount of Green Blue    & 133 & it12-gb \\
Amount of Blue          & 134 & it12-b  \\
Amount of Purple Blue   & 135 & it12-pb \\
Amount of Purple        & 136 & it12-p  \\
Amount of Purple Red    & 137 & it12-pr \\

\midrule
\multicolumn{3}{c}{Rule of Thirds} \\
\midrule
Average of central Colourfulness                 & 138 & r3-cs-avg   \\
Standard Deviation of central Colourfulness      & 139 & r3-cs-std   \\
Average of central Hue                           & 140 & r3-h-avg    \\
Standard Deviation of central Hue                & 141 & r3-h-std    \\
Average of central Lightness                     & 142 & r3-l-avg    \\
Standard Deviation of central Lightness          & 143 & r3-l-std    \\
Average of central Saturation$_{HSL}$            & 144 & r3-shsl-avg \\
Standard Deviation of central Saturation$_{HSL}$ & 145 & r3-shsl-std \\
Average of central Saturation$_{HSV}$            & 146 & r3-shsv-avg \\
Standard Deviation of central Saturation$_{HSV}$ & 147 & r3-shsv-std \\
Average of central Value                         & 148 & r3-v-avg    \\
Standard Deviation of central Value              & 149 & r3-v-std    \\

\midrule
\multicolumn{3}{c}{Itten Contrast of$\ldots$} \\
\midrule
Saturation$_{HSL}$ (rule of thirds)              & 150 & itct-sat-sl-r3      \\
Saturation$_{HSL}$ (rule of sevenths)            & 151 & itct-sat-sl-r7      \\
Saturation$_{HSL}$ (watershed)                   & 152 & itct-sat-sl-sg      \\
Saturation$_{HSV}$ (rule of thirds)              & 153 & itct-sat-sv-r3      \\
Saturation$_{HSV}$ (rule of sevenths)            & 154 & itct-sat-sv-r7      \\
Saturation$_{HSV}$ (watershed)                   & 155 & itct-sat-sv-sg      \\
Light and Dark of Lightness (rule of thirds)     & 156 & itct-lightdark-l-r3 \\
Light and Dark of Lightness (rule of sevenths)   & 157 & itct-lightdark-l-r7 \\
Light and Dark of Lightness (watershed)          & 158 & itct-lightdark-l-sg \\
Light and Dark of Value (rule of thirds)         & 159 & itct-lightdark-v-r3 \\
Light and Dark of Value (rule of sevenths)       & 160 & itct-lightdark-v-r7 \\
Light and Dark of Value (watershed)              & 161 & itct-lightdark-v-sg \\
Hue (rule of thirds)                             & 162 & itct-hue-h-r3       \\
Hue (rule of sevenths)                           & 163 & itct-hue-h-r7       \\
Hue (watershed)                                  & 164 & itct-hue-h-sg       \\
Hue of Colourfulness (rule of thirds)            & 165 & itct-hue-cs-r3      \\
Hue of Colourfulness (rule of sevenths)          & 166 & itct-hue-cs-r7      \\
Hue of Colourfulness (watershed)                 & 167 & itct-hue-cs-sg      \\
Warm and Cold of Hue (rule of thirds)            & 168 & itct-warmcold-h-r3  \\
Warm and Cold of Hue (rule of sevenths)          & 169 & itct-warmcold-h-r7  \\
Warm and Cold of Hue (watershed)                 & 170 & itct-warmcold-h-sg  \\
Warm and Cold of Colourfulness (rule of thirds)  & 171 & itct-warmcold-cs-r3 \\
Warm and Cold of Colourfulness (rule of sevenths)& 172 & itct-warmcold-cs-r7 \\
Warm and Cold of Colourfulness (watershed)       & 173 & itct-warmcold-cs-sg \\
Complements of Hue (rule of thirds)              & 174 & itct-comp-h-r3      \\
Complements of Hue (rule of sevenths)            & 175 & itct-comp-h-r7      \\
Complements of Hue (watershed)                   & 176 & itct-comp-h-sg      \\
Complements of Colourfulness (rule of thirds)    & 177 & itct-comp-cs-r3     \\
Complements of Colourfulness (rule of sevenths)  & 178 & itct-comp-cs-r7     \\
Complements of Colourfulness (watershed)         & 179 & itct-comp-cs-sg     \\

\midrule
\multicolumn{3}{c}{Image Averages} \\
\midrule
Average of Colourfulness                 & 180 & cs-avg   \\
Standard Deviation of Colourfulness      & 181 & cs-std   \\
Average of Hue                           & 182 & h-avg    \\
Standard Deviation of Hue                & 183 & h-std    \\
Average of Lightness                     & 184 & l-avg    \\
Standard Deviation of Lightness          & 185 & l-std    \\
Average of Saturation$_{HSL}$            & 186 & shsl-avg \\
Standard Deviation of Saturation$_{HSL}$ & 187 & shsl-std \\
Average of Saturation$_{HSV}$            & 188 & shsv-avg \\
Standard Deviation of Saturation$_{HSV}$ & 189 & shsv-std \\
Average of Value                         & 190 & v-avg    \\
Standard Deviation of Value              & 191 & v-std    \\
\bottomrule
\end{longtable}
\end{center}

\newpage

\begin{center}
\footnotesize
\begin{longtable}{l|rr|rr|rr}
\toprule
\multirow{2}{*}{Feature}
  & \multicolumn{2}{c}{renaissance}
  & \multicolumn{2}{c}{baroque}
  & \multicolumn{2}{c}{neoclassicism}
  \\
\cline{2-7}
& mean & std & mean & std & mean & std \\
\midrule
\endhead

\midrule
\multicolumn{3}{r}{} \\
\multicolumn{3}{r}{{Continued on next page}} \\
\multicolumn{3}{r}{} \\
\midrule
\endfoot

\bottomrule
\caption[Feature values (first half)]{Mean and standard deviation of all
features grouped by the art schools of renaissance, baroque and neoclassicism.
The features are identified by the short names from table \ref{tab:glossary}.}
\label{tab:bigtop}
\endlastfoot

\input{means-stds-top.tex}
\bottomrule
\end{longtable}
\end{center}

\newpage

\begin{center}
\footnotesize
\begin{longtable}{l|rr|rr||rr}
\toprule
\multirow{2}{*}{Feature}
  & \multicolumn{2}{c}{romanticism}
  & \multicolumn{2}{c}{impressionism}
  & \multicolumn{2}{c}{mughal}
  \\
\cline{2-7}
& mean & std & mean & std & mean & std \\
\midrule
\endhead

\midrule
\multicolumn{3}{r}{} \\
\multicolumn{3}{r}{{Continued on next page}} \\
\multicolumn{3}{r}{} \\
\midrule
\endfoot

\bottomrule
\caption[Feature values (second half)]{Mean and standard deviation of all
features grouped by the art schools of romanticism and impressionism.  It also
shows the values for mughal miniature paintings.  The features are identified
by the short names from table \ref{tab:glossary}.}
\label{tab:bigbot}
\endlastfoot

\input{means-stds-bot.tex}
\bottomrule
\end{longtable}
\end{center}

\chapter{Code}

The electronic version of this report contains below the most important pieces
of the code used to perform the experiments.  The hard copy of the report
contains a CD with the full code for the experiments and for this document.
Appendix \ref{chap:resources} also explains how to acquire the resources on the
CD if the CD is lost.

{\footnotesize
\centerline{\fbox{\large \texttt{step01-normalise.sh: size normalises images}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step01-normalise.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{norm-size.pl: outputs new sizes for image}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/norm-size.pl}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step02-hsl-hsv.sh: separates channels}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step02-hsl-hsv.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{sv2cs.py: calculates colourfulness}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/sv2cs.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{sobel.py: applies Sobel filter}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/sobel.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{canny.py: applies Canny filter}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/canny.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step06-jpeg.sh: JPEG compression}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step06-jpeg.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step07-kolmogorov.sh: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step07-kolmogorov.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{glcm.py: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/glcm.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{imgavg.py: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/imgavg.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{chops.py: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/chops.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{colour.py: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/colour.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{itten.py: feature extractor}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/itten.py}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step13-metadata.sh: extra metadata}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step13-metadata.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{util/mksvmf.pl: metadata translator}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/mksvmf.pl}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step14-libsvm-format.sh: dataset joiner}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step14-libsvm-format.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{step15-classify.sh: runs classifiers}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/step15-classify.sh}
}
{\footnotesize
\centerline{\fbox{\large \texttt{util/table-compm.py: output table builder}}}
\VerbatimInput[ frame=leftline
              , numbers=left ]{../../src/util/table-compm.py}
}

\newpage
\null
\thispagestyle{empty}
\newpage

\end{document}

